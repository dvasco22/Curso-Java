#https://www.youtube.com/watch?v=iX_on3VxZzk
#python -m http.server 8000
#http://localhost:8000/index2.html
#tensorflowjs_converter --input_format keras --output_format ctof.h5 ./


# Este ejemplo es cl√°sico en TensorFlow para ense√±ar c√≥mo una red neuronal puede aprender
# relaciones matem√°ticas simples, como una conversi√≥n lineal. esto es 
#
#         F = C * 1.8 + 32  ( y = mx + b)
#
# En el aprendizaje automatico no se conoce la regla de conversion se infiere a partir 
# de entradas y resultados con la finalidad que el modelo aprenda por si solo el algoritmo
# A pesar de que una red neuronal es excesiva para este problema, que se puede predecir
# a trav√©s de una recta, sirve como introducci√≥n al proceso de entrenamiento y predicci√≥n.

#üîπ1. Importaci√≥n de librer√≠as
#     tensorflow: 
#        biblioteca principal para machine learning. Aqu√≠ se usa para crear, 
#        entrenar y predecir con redes neuronales.
#     numpy: 
#        se utiliza para manipular arrays num√©ricos, aunque en este c√≥digo 
#        solo se usa para hacer una predicci√≥n.
#     matplotlib.pyplot: 
#        usada para graficar, en este caso, para visualizar la evoluci√≥n del 
#        error (p√©rdida) durante el entrenamiento.
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt


#üîπ2. Datos de entrenamiento : Se definen dos tensores:
#       celsius: entradas (features)
#       fahrenheit: salidas esperadas (labels)
#     Estos pares representan valores conocidos de conversi√≥n de temperaturas. 
#     Son los datos que la red usar√° para aprender la relaci√≥n.
celsius = tf.constant([-40, -10, 0, 8, 15, 22, 38], dtype=tf.float32)
fahrenheit = tf.constant([-40, 14, 32, 46.4, 59, 71.6, 100.4], dtype=tf.float32)

#üîπ3. Definici√≥n del modelo (Red neuronal)
#     Se construye una red neuronal con 3 capas:
#      3.1 Capa oculta-1:    --> capa densa con 3 neuronas y una entrada
#                                units=3: 3 neuronas
#                                input_shape=[1]:  --> entrada temperatura en Celsius)
#      3.2 Capa oculta-2:    --> Otra capa densa con 3 neuronas
#      3.4 Capa de salida:
#          1 neurona (produce un solo valor: la temperatura en Fahrenheit)
#      Sequential indica que las capas est√°n conectadas una tras otra, en orden.
#
# Nota: en el c√≥digo original hay una versi√≥n comentada con una sola capa; 
# aqu√≠ se usa una arquitectura m√°s profunda para efectos did√°cticos o experimentales.

capa = tf.keras.layers.Dense(units=1, input_shape=[1])
modelo = tf.keras.Sequential([capa])
# oculta1 = tf.keras.layers.Dense(units=3, input_shape=[1])
# oculta2 = tf.keras.layers.Dense(units=3)
# salida = tf.keras.layers.Dense(units=1)
# modelo = tf.keras.Sequential([oculta1, oculta2, salida])

#üîπ4. Compilaci√≥n del modelo
#     Optimizador: Adam con tasa de aprendizaje de 0.1. 
#                 Se encarga de actualizar los pesos de la red para reducir el error.
#     Funci√≥n de p√©rdida: 'mean_squared_error' 
#                (error cuadr√°tico medio), adecuada para problemas de regresi√≥n l√≠neal.
modelo.compile(
    optimizer=tf.keras.optimizers.Adam(0.1),
    loss='mean_squared_error'
)


#üîπ5. Entrenamiento del modelo
#     Se entrena el modelo usando los datos por 100 √©pocas (iteraciones).
#     verbose=False suprime la salida durante el entrenamiento.
#     El historial devuelto contiene informaci√≥n √∫til como el valor de p√©rdida por √©poca.
print("Comenzando entrenamiento...")
historial = modelo.fit(celsius, fahrenheit, epochs=1000, verbose=False)
print("Modelo entrenado!")


#üîπ6. Visualizaci√≥n del error durante el entrenamiento
#     Se grafica c√≥mo la p√©rdida disminuye a lo largo de las √©pocas, 
#     lo cual es una se√±al de que el modelo est√° aprendiendo.
#     en este caso a partir de la √©poca 60 el modelo esta completamente entrenado
plt.xlabel("# Epoca")
plt.ylabel("Magnitud de p√©rdida")
plt.plot(historial.history["loss"])

#üîπ7. Predicci√≥n
#     Se prueba el modelo con un nuevo valor: 100 grados Celsius.
#     Se imprime la predicci√≥n, que deber√≠a estar cerca de 212¬∞F (la conversi√≥n exacta).
print("Hagamos una predicci√≥n!")
resultado = modelo.predict(np.array([100.0]))
print("El resultado es " + str(resultado) + " fahrenheit!")

#üîπ8. Inspecci√≥n de pesos del modelo
#     Se imprimen los pesos y sesgos aprendidos por cada capa, que son los par√°metros 
#     ajustados durante el entrenamiento. Esta informaci√≥n muestra c√≥mo internamente 
#     el modelo est√° representando la relaci√≥n Celsius ‚Üí Fahrenheit.
print("Variables internas del modelo")
print(capa.get_weights())
# print(oculta1.get_weights())
# print(oculta2.get_weights())
# print(salida.get_weights())

#Exportar el modelo en formato h5
#modelo.save('celsius_a_fahrenheit.h5')


# üî∑ La tasa de aprendizaje es un valor (por lo general peque√±o) que controla el tama√±o de los pasos que da el 
# optimizador al ajustar los pesos del modelo durante el entrenamiento.
#
# F√≥rmula simplificada de actualizaci√≥n:
#     nuevo_peso = peso_actual - tasa_de_aprendizaje √ó gradiente
#
# üîπ Una tasa muy peque√±a (p. ej. 0.0001) provoca un prendizaje muy estable y preciso pero puede tomar muchas √©pocas en converger
# Puede quedarse atrapado en m√≠nimos locales si no tiene suficiente impulso.
#
# üîπPoe el contrario una tasa muy grande (p. ej. 0.1, 0.5 o m√°s) al principio aprende r√°pido pero puede saltarse el M√≠nimo o√≥ptimo
#    con riesgo de inestabilidad y las p√©rdidas oscilan o incluso aumentan.
# Una tasa media conlleva a pasos controlados acelera el proceso de aprendizaje pero sin saltarte el m√≠nimo.





