#https://www.youtube.com/watch?v=iX_on3VxZzk
#python -m http.server 8000
#http://localhost:8000/index2.html
#tensorflowjs_converter --input_format keras --output_format ctof.h5 ./


# Este ejemplo es clÃ¡sico en TensorFlow para enseÃ±ar cÃ³mo una red neuronal puede aprender
# relaciones matemÃ¡ticas simples, como una conversiÃ³n lineal. esto es 
#
#         F = C * 1.8 + 32  ( y = mx + b)
#
# En el aprendizaje automatico no se conoce la regla de conversion se infiere a partir 
# de entradas y resultados con la finalidad que el modelo aprenda por si solo el algoritmo
# A pesar de que una red neuronal es excesiva para este problema, que se puede predecir
# a travÃ©s de una recta, sirve como introducciÃ³n al proceso de entrenamiento y predicciÃ³n.

#ğŸ”¹1. ImportaciÃ³n de librerÃ­as
#     tensorflow: 
#        biblioteca principal para machine learning. AquÃ­ se usa para crear, 
#        entrenar y predecir con redes neuronales.
#     numpy: 
#        se utiliza para manipular arrays numÃ©ricos, aunque en este cÃ³digo 
#        solo se usa para hacer una predicciÃ³n.
#     matplotlib.pyplot: 
#        usada para graficar, en este caso, para visualizar la evoluciÃ³n del 
#        error (pÃ©rdida) durante el entrenamiento.
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt


#ğŸ”¹2. Datos de entrenamiento : Se definen dos tensores:
#       celsius: entradas (features)
#       fahrenheit: salidas esperadas (labels)
#     Estos pares representan valores conocidos de conversiÃ³n de temperaturas. 
#     Son los datos que la red usarÃ¡ para aprender la relaciÃ³n.
celsius = tf.constant([-40, -10, 0, 8, 15, 22, 38], dtype=tf.float32)
fahrenheit = tf.constant([-40, 14, 32, 46.4, 59, 71.6, 100.4], dtype=tf.float32)

#ğŸ”¹3. DefiniciÃ³n del modelo (Red neuronal)
#     Se construye una red neuronal con 3 capas:
#      3.1 Capa oculta-1:    --> capa densa con 3 neuronas y una entrada
#                                units=3: 3 neuronas
#                                input_shape=[1]:  --> entrada temperatura en Celsius)
#      3.2 Capa oculta-2:    --> Otra capa densa con 3 neuronas
#      3.4 Capa de salida:
#          1 neurona (produce un solo valor: la temperatura en Fahrenheit)
#      Sequential indica que las capas estÃ¡n conectadas una tras otra, en orden.
#
# Nota: en el cÃ³digo original hay una versiÃ³n comentada con una sola capa; 
# aquÃ­ se usa una arquitectura mÃ¡s profunda para efectos didÃ¡cticos o experimentales.

capa = tf.keras.layers.Dense(units=1, input_shape=[1])
modelo = tf.keras.Sequential([capa])
# oculta1 = tf.keras.layers.Dense(units=3, input_shape=[1])
# oculta2 = tf.keras.layers.Dense(units=3)
# salida = tf.keras.layers.Dense(units=1)
# modelo = tf.keras.Sequential([oculta1, oculta2, salida])

#ğŸ”¹4. CompilaciÃ³n del modelo
#     Optimizador: Adam con tasa de aprendizaje de 0.1. 
#                 Se encarga de actualizar los pesos de la red para reducir el error.
#     FunciÃ³n de pÃ©rdida: 'mean_squared_error' 
#                (error cuadrÃ¡tico medio), adecuada para problemas de regresiÃ³n lÃ­neal.
modelo.compile(
    optimizer=tf.keras.optimizers.Adam(0.1),
    loss='mean_squared_error'
)


#ğŸ”¹5. Entrenamiento del modelo
#     Se entrena el modelo usando los datos por 100 Ã©pocas (iteraciones).
#     verbose=False suprime la salida durante el entrenamiento.
#     El historial devuelto contiene informaciÃ³n Ãºtil como el valor de pÃ©rdida por Ã©poca.
print("Comenzando entrenamiento...")
historial = modelo.fit(celsius, fahrenheit, epochs=1000, verbose=False)
print("Modelo entrenado!")


#ğŸ”¹6. VisualizaciÃ³n del error durante el entrenamiento
#     Se grafica cÃ³mo la pÃ©rdida disminuye a lo largo de las Ã©pocas, 
#     lo cual es una seÃ±al de que el modelo estÃ¡ aprendiendo.
#     en este caso a partir de la Ã©poca 60 el modelo esta completamente entrenado
plt.xlabel("# Epoca")
plt.ylabel("Magnitud de pÃ©rdida")
plt.plot(historial.history["loss"])

#ğŸ”¹7. PredicciÃ³n
#     Se prueba el modelo con un nuevo valor: 100 grados Celsius.
#     Se imprime la predicciÃ³n, que deberÃ­a estar cerca de 212Â°F (la conversiÃ³n exacta).
print("Hagamos una predicciÃ³n!")
resultado = modelo.predict(np.array([100.0]))
print("El resultado es " + str(resultado) + " fahrenheit!")

#ğŸ”¹8. InspecciÃ³n de pesos del modelo
#     Se imprimen los pesos y sesgos aprendidos por cada capa, que son los parÃ¡metros 
#     ajustados durante el entrenamiento. Esta informaciÃ³n muestra cÃ³mo internamente 
#     el modelo estÃ¡ representando la relaciÃ³n Celsius â†’ Fahrenheit.
print("Variables internas del modelo")
print(capa.get_weights())
# print(oculta1.get_weights())
# print(oculta2.get_weights())
# print(salida.get_weights())

#Exportar el modelo en formato h5
#modelo.save('celsius_a_fahrenheit.h5')


# ğŸ”¹ La tasa de aprendizaje es un valor (por lo general pequeÃ±o) que controla el tamaÃ±o de los pasos que da el 
# optimizador al ajustar los pesos del modelo durante el entrenamiento.
#
#      FÃ³rmula simplificada de actualizaciÃ³n:
#         nuevo_peso = peso_actual - tasa_de_aprendizaje Ã— gradiente
#
# ğŸ”¹ Una tasa muy pequeÃ±a (p. ej. 0.0001) provoca un prendizaje muy estable y preciso pero puede tomar muchas Ã©pocas en converger
# Puede quedarse atrapado en mÃ­nimos locales si no tiene suficiente impulso.
#
# ğŸ”¹Por el contrario una tasa muy grande (p. ej. 0.1, 0.5 o mÃ¡s) al principio aprende rÃ¡pido pero puede saltarse el MÃ­nimo Ã³ptimo
#    con riesgo de inestabilidad y las pÃ©rdidas oscilan o incluso aumentan.
# ğŸ”¹ Una tasa media conlleva a pasos controlados acelera el proceso de aprendizaje pero sin saltarte el mÃ­nimo.





